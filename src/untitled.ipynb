{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from config import WEBSITE, BASE_URL, extract_series_info\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tournament_matches(tour_url):\n",
    "    page = requests.get(tour_url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    tour_match_list = []\n",
    "    for grp in soup.find_all('div',{'class':'cb-col-100'}):\n",
    "        if grp.find('div',{'class':'schedule-date'}) != None:\n",
    "            schedule_object = grp.find('div',{'class':'schedule-date'}).find_all('span')\n",
    "            if len(schedule_object) != 0:\n",
    "                match_schedule_ts = [x['ng-bind'].split('|')[0].strip()[:-3] for x in schedule_object]\n",
    "                match_schedule = [datetime.utcfromtimestamp(int(x)).strftime('%Y-%m-%d') for x in match_schedule_ts]\n",
    "                if len(match_schedule) == 2:\n",
    "                    match_days = str(match_schedule[0]) + \" | \" + str(match_schedule[1])\n",
    "                else: \n",
    "                    match_days = str(match_schedule[0])\n",
    "            match_details_object = grp.find('div',attrs={'class':'cb-col-60'})\n",
    "            # handling match_href issue\n",
    "            match_href_object = match_details_object.find('a',attrs={'class':\"text-hvr-underline\"})\n",
    "            if match_href_object is None:\n",
    "                break\n",
    "            else:\n",
    "                match_href = match_href_object['href']\n",
    "            # handling None issue for future matches\n",
    "            match_result_object = match_details_object.find('a',attrs={'class':\"cb-text-complete\"})\n",
    "            match_result = match_result_object.text if match_result_object != None else None\n",
    "            row = {\n",
    "                'match_days' : match_days,\n",
    "                'match_href' : WEBSITE + match_details_object.find('a',attrs={'class':\"text-hvr-underline\"})['href'],\n",
    "                'match_title' : match_details_object.find('a',attrs={'class':\"text-hvr-underline\"}).text,\n",
    "                'match_result' : match_result,\n",
    "                'match_ground' : match_details_object.div.text.split(\", \")[0],\n",
    "                'match_location' : match_details_object.div.text.split(\", \")[1]           \n",
    "            }\n",
    "            tour_match_list.append(row)   \n",
    "    return tour_match_list\n",
    "\n",
    "def run(first_iteration = True):\n",
    "    error_url = []\n",
    "    if first_iteration:\n",
    "        series_df = pd.read_csv(\"../data_dir/tournament_tb.csv\")\n",
    "        tour_match_df = pd.DataFrame(None,columns=['match_days','match_href','match_title',\n",
    "                                              'match_result','match_ground', 'match_location'])\n",
    "        first_iteration=False\n",
    "    for idx, row in series_df.iterrows():\n",
    "        print(row['title'])\n",
    "#         if idx%10 == 0:\n",
    "#             print('timing out for 30 seconds...')\n",
    "#             time.sleep(30)\n",
    "        try:\n",
    "            tour_match_df = tour_match_df.append(extract_tournament_matches(row['href']),ignore_index=True)\n",
    "        except:\n",
    "            try:\n",
    "                time.sleep(30)\n",
    "                tour_match_df = tour_match_df.append(extract_tournament_matches(row['href']),ignore_index=True)\n",
    "            except:\n",
    "                print('failed in second attempt as well')\n",
    "                print(row['href'])\n",
    "                error_url.append(row['href'])\n",
    "    # trying the errored one again to resolve the connectivity issue\n",
    "    error_list_empty, max_loop = False, 10\n",
    "    i = 1\n",
    "    while (not error_list_empty) and (i != max_loop):\n",
    "        print('RETRYING ATTEMPT - ', i) \n",
    "        new_error_url = []\n",
    "        for url in error_url:\n",
    "            try:\n",
    "                tour_match_df = tour_match_df.append(extract_tournament_matches(url),ignore_index=True)\n",
    "            except:\n",
    "                print('failed url :', url)\n",
    "                new_error_url.append(url)\n",
    "        if len(new_error_url) == 0:\n",
    "            error_list_empty = True\n",
    "        else:\n",
    "            error_url = new_error_url\n",
    "            i += 1\n",
    "            time.sleep(30)\n",
    "    tour_match_df.to_csv(\"../data_dir/tour_match_tb.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
